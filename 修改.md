## 修改

### HDFS

#### hdfs-site.xml

```xml
<property>
   <name>dfs.permissions.superusergroup</name>
   <value>hadoop</value>
   <description>The name of the group of super-users. The value should be a sin$
</property>
<property>
   <name>dfs.replication</name>
   <value>3</value>
</property>
<property>
     <name>dfs.nameservices</name>
     <value>hadoop-ha</value>
</property>
<property>
     <name>dfs.ha.namenodes.hadoop-ha</name>
     <value>nn1,nn2</value>
</property>
<property>
     <name>dfs.namenode.http-address.hadoop-ha.nn1</name>
     <value>master1:50070</value>
</property>
<property>
     <name>dfs.namenode.rpc-address.hadoop-ha.nn2</name>
     <value>worker1:8020</value>
</property>
<property>
     <name>dfs.namenode.http-address.hadoop-ha.nn2</name>
     <value>worker1:50070</value>
</property>
<property>
     <name>dfs.namenode.shared.edits.dir</name>
     <value>qjournal://master1:8485;worker1:8485;worker2:8485/hadoop-ha</value>
</property>
<property>
     <name>dfs.journalnode.edits.dir</name>
     <value>/home/hadoop/journalnode</value>
</property>
<property>
     <name>dfs.ha.fencing.methods</name>
     <value>sshfence</value>
</property>
<property>
     <name>dfs.ha.fencing.ssh.private-key-files</name>
     <value>/home/hadoop/.ssh/id_rsa</value>
</property>
<property>
     <name>dfs.client.failover.proxy.provider.hadoop-ha</name>
     <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyPr$
</property>
<property>
     <name>dfs.ha.automatic-failover.enabled</name>
     <value>true</value>
</property>

```

```
scp hdfs-site.xml worker1:/usr/local/hadoop/etc/hadoop
scp hdfs-site.xml worker2:/usr/local/hadoop/etc/hadoop
scp hdfs-site.xml worker3:/usr/local/hadoop/etc/hadoop
```



#### core-site.xml

```xml
<property>
   <name>hadoop.tmp.dir</name>
   <value>/home/hadoop/data</value>
   <description>Temporary Directory.</description>
</property>
<property>
   <name>fs.defaultFS</name>
   <value>hdfs://hadoop-ha</value>
   <description>Use HDFS as file storage engine</description>
</property>

<property>
   <name>ha.zookeeper.quorum</name>
   <value>master:2181,worker1:2181,worker2:2181</value>
</property>
<property>
   <name>hadoop.proxyuser.hadoop.hosts</name>
   <value>*</value>
</property>
<property>
   <name>hadoop.proxyuser.hadoop.groups</name>
   <value>*</value>
</property>



```

```
 scp core-site.xml worker1:/usr/local/hadoop/etc/hadoop
 scp core-site.xml worker2:/usr/local/hadoop/etc/hadoop
 scp core-site.xml worker3:/usr/local/hadoop/etc/hadoop
```

#### 建資料夾(在journalnode節點上,master,worker1,worker2)

```
mkdir ~/journalnode
```

```
ssh worker1
mkdir ~/journalnode
exit
```

```
ssh worker2
mkdir ~/journalnode
exit
```

#### 格式化

啟動journalnode,在master,worker1,worker2上均要執行

```\
hadoop-daemon.sh start journalnode
```

請先確定data 以及journalnode是否為空

```
hdfs namenode -format
```

在master上啟動namenode

```
hadoop-daemon.sh start namenode
```

在worker 上啟動namenode

```
hadoop-daemon.sh start namenode
```

重新啟動hdfs

```
stop-dfs.sh
start-dfs.sh
```

確認hadoop系統,在master

```
hdfs haadmin -getServiceState nn1
hdfs haadmin -getServiceState nn2
```

將master設定為主要的namenode,在master上執行

```
hdfs haadmin -transitionToActive --forcemanual nn1
```

將worker1更改為standby

```
hdfs haadmin -transitionToStandby --forcemanual nn2
```



### YARN

#### yarn-site.xml

```xml
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>

<property>
   <name>yarn.scheduler.minimum-allocation-mb</name>
   <value>1024</value>
</property>

<property>
   <name>yarn.scheduler.maximum-allocation-mb</name>
   <value>1024</value>
</property>

<property>
   <name>yarn.nodemanager.resource.memory-mb</name>
   <value>1024</value>
</property>

<property>
   <name>yarn.app.mapreduce.am.resource.mb</name>
   <value>1024</value>
</property>

<property>
   <name>yarn.nodemanager.vmem-check-enabled</name>
   <value>false</value>
</property>

<property>
   <name>yarn.resourcemanager.ha.enabled</name>
   <value>true</value>
</property>

<property>
   <name>yarn.resourcemanager.cluster-id</name>
   <value>rm_ha</value>
</property>

<property>
   <name>yarn.resourcemanager.ha.rm-ids</name>
   <value>rm1,rm2</value>
</property>

<property>
   <name>yarn.resourcemanager.hostname.rm1</name>
   <value>master</value>
</property>

<property>
   <name>yarn.resourcemanager.hostname.rm2</name>
   <value>worker1</value>
</property>

<property>
   <name>yarn.resourcemanager.recovery.enabled</name>
   <value>true</value>
</property>

<property>
   <name>yarn.resourcemanager.store.class</name>
   <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>

<property>
   <name>yarn.resourcemanager.zk-address</name>
   <value>master:2181,worker1:2181,worker2:2181</value>
</property>

<property>
    <name>yarn.resourcemanager.address.rm1</name>
    <value>master:8032</value>
</property>

<property>
    <name>yarn.resourcemanager.scheduler.address.rm1</name>
    <value>master:8034</value>
</property>

<property>
    <name>yarn.resourcemanager.webapp.address.rm1</name>
    <value>master:8088</value>
</property>

<property>
    <name>yarn.resourcemanager.address.rm2</name>
    <value>worker1:8032</value>
</property>

<property>
    <name>yarn.resourcemanager.scheduler.address.rm2</name>
    <value>worker1:8034</value>
</property>

<property>
    <name>yarn.resourcemanager.webapp.address.rm2</name>
    <value>worker1:8088</value>
</property>
  
```

**yarn.nodemanager.vmem-check-enabled** 調為 false 表示不限制 YARN 啟動的container內存

將 yarn-site.xml 複製到所以機器

```
scp yarn-site.xml worker1:/usr/local/hadoop/etc/hadoop
scp yarn-site.xml worker2:/usr/local/hadoop/etc/hadoop
scp yarn-site.xml worker3:/usr/local/hadoop/etc/hadoop
```

#### mapred-site.xml

```xml
<property>
        <name>mapreduce.jobhistory.address</name>
        <value>worker2:10020</value>
</property>

<property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>worker2:19888</value>
</property>

```

複製到其他台機器

```
scp mapred-site.xml worker1:/usr/local/hadoop/etc/hadoop
scp mapred-site.xml worker2:/usr/local/hadoop/etc/hadoop
scp mapred-site.xml worker3:/usr/local/hadoop/etc/hadoop
```



### 結合

在 master , worker1 , worker2 啟動 zookeeper

```
zkServer.sh start
```

格式化

```
hdfs zkfc -formatZK
```







如果在hdfs-site.xml裡加入,可以啟動webhdfs

```
<property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
</property>
```

在terminal 執行 可以put文件

```
curl -i -X PUT -T /home/hadoop/test1.txt "http://worker2:50075/webhdfs/v1/user/hadoop/test3.txt?op=CREATE&user.name=hadoop&namenoderpcaddress=worker1:8020&createflag=&createparent=true&overwrite=true"

```



spark shell問題

如果出現

```
WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
```

在spark的conf/spark-env.sh文件加入

```
export  LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
```

在~/.bashrc加入

```
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
```

```
source ~/.bashrc
```

重啟spark-shell

```
./bin/spark-submit --class wordcount --master local[*] /home/hadoop/sbttest_2.11-0.1.jar input.txt ./output
```

