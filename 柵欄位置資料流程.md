## 柵欄位置資料



### API

1. 建立一個資料夾

   ```
   mkdir -p docker_v/code
   ```

2. 在 code 資料夾存放一個 python 檔案

   ```python
   from flask import Flask, request
   from kafka import KafkaProducer
   import os
   import os.path
   import time
   import json
   
   app = Flask(__name__)
   
   
   
   @app.route('/data_to_kafka', methods=['GET','POST'])
   def get_data():
   	if request.method == 'POST':
   		data = request.json["records"]
   		producer = KafkaProducer(bootstrap_servers='192.168.1.114',value_serializer=lambda v: json.dumps(v).encode('utf-8'))
   		json_data = sorted(data, key=lambda k: k['date'])
   		for i in range(len(json_data)):
   			producer.send('spark_kafka', json_data[i])
   			time.sleep(0.01)
   			print(json_data[i])
   	return 'sucessful'
   
   
   port = os.getenv('PORT', '7500')
   
   if __name__ == "__main__":
   	app.run(host='0.0.0.0', port=int(port))
   
   ```

3. 在code 資料夾存放一個 requirements.txt 檔案

   ```
   kafka-python
   flask
   ```

4. 請先安裝好docker

5. 啟動 API

   下載 python 的 docker image

   ```
   docker pull python
   ```

   開啟 container

   ```docker
   docker run -it --rm -p 7500:7500 -v /home/hduser/docker_v/:/test_flask python:latest bash
   ```

   啟動 API

   ```shell
   python /test_flask/code/API_to_kafka.py
   ```



### Spark

1. 建立 spark 程式

   ```scala
   import org.apache.spark.sql.functions._
   import java.sql._
   import org.apache.spark.sql.{ForeachWriter, Row}
   import org.apache.spark.sql.{ForeachWriter, Row, SparkSession}
   import org.apache.spark.sql.streaming.ProcessingTime
   import spark.implicits._
   import org.apache.spark.sql.types._
   
   class jdbcSink(url:String,user:String,pwd:String) extends ForeachWriter[Row]{
     val driver = "com.mysql.jdbc.Driver";
     var statement:Statement = _;
     var connection:Connection  = _;
   
     def open(partitionId: Long, version: Long): Boolean = {
       Class.forName(driver);
       connection = DriverManager.getConnection(url,user,pwd);
       this.statement = connection.createStatement();
   
       true;
     }
     override def process(value: Row): Unit = {
       val date = value.getAs[String]("date")
       val tagId = value.getAs[String]("tagId").toInt
       val antennaNo = value.getAs[String]("antennaNo")
   
       statement.executeUpdate("insert into raw_data(date,tagId,antennaNo) values('"+date+"',"+tagId+",'"+antennaNo+"')")
       if(antennaNo=="A"){
         statement.executeUpdate("replace into status_data(date,tagId,status) values('"+date+"',"+tagId+",'out')")
       }else{
         statement.executeUpdate("replace into status_data(date,tagId,status) values('"+date+"',"+tagId+",'in')")
       }
     }
   
     override def close(errorOrNull: Throwable): Unit = {
       connection.close()
     }
   }
   
   val df = spark.readStream
     .format("kafka")
     .option("kafka.bootstrap.servers","192.168.1.114:9092")
     .option("startingOffsets", "latest")
     .option("subscribe","spark_kafka")
     .option("maxOffsetsPerTrigger", 10000)
     .load()
   
   val kafkaDF = df.selectExpr("CAST(value AS STRING)")
   
   val schema = new StructType()
     .add("date",StringType)
     .add("tagId",StringType)
     .add("antennaNo",StringType)
   
   val kafkaDF2 = kafkaDF.select(from_json($"value", schema) as "data1").select($"data1.*")
   
   
   val url = "jdbc:mysql://192.168.1.115:3306/hidb?useSSL=false"
   val user  = "root"
   val pwd = "mysql"
   val writer:ForeachWriter[Row] = new jdbcSink(url,user,pwd);
   val query = kafkaDF2
     .writeStream
     .foreach(writer)
     .outputMode("update")
     .start()
   query.awaitTermination()
   
   
   
   ```

2. 啟動 spark 程式

   ```shell
   bin/spark-shell --driver-class-path ../kafka-spark-stream-app/jars/mysql-connector-java-5.1.47.jar --jars ../kafka-spark-stream-app/jars/mysql-connector-java-5.1.47.jar,../kafka-spark-stream-app/jars/spark-streaming-kafka-0-10_2.11-2.4.3.jar,../kafka-spark-stream-app/jars/kafka-clients-2.2.1.jar,../kafka-spark-stream-app/jars/spark-sql-kafka-0-10_2.11-2.4.3.jar -i ../kafka-spark-stream-app/demo3.scala
   ```
   
3. 移欄資料版本

   ```scala
   import org.apache.spark.sql.functions._
   import java.sql._
   import org.apache.spark.sql.{ForeachWriter, Row}
   import org.apache.spark.sql.{ForeachWriter, Row, SparkSession}
   import org.apache.spark.sql.streaming.ProcessingTime
   import spark.implicits._
   import org.apache.spark.sql.types._
   
   class jdbcSink(url:String,user:String,pwd:String) extends ForeachWriter[Row]{
     val driver = "com.mysql.jdbc.Driver";
     var statement:Statement = _;
     var connection:Connection  = _;
   
     def open(partitionId: Long, version: Long): Boolean = {
       Class.forName(driver);
       connection = DriverManager.getConnection(url,user,pwd);
       this.statement = connection.createStatement();
   
       true;
     }
     override def process(value: Row): Unit = {
       val ID =if(value.getAs[String]("ID")==null) "NULL" else "'"+value.getAs[String]("ID")+"'"
       val SeqNo = if(value.getAs[String]("SeqNo")==null) "NULL" else value.getAs[String]("SeqNo").toInt
       val FormID = if(value.getAs[String]("FormID")==null) "NULL" else "'"+value.getAs[String]("FormID")+"'"
       val ExecDate = if(value.getAs[String]("ExecDate")==null) "NULL" else "'"+value.getAs[String]("ExecDate")+"'"
       val ExecEmpNo = if(value.getAs[String]("ExecEmpNo")==null) "NULL" else "'"+value.getAs[String]("ExecEmpNo")+"'"
       val MovingType = if(value.getAs[String]("MovingType")==null) "NULL" else "'"+value.getAs[String]("MovingType")+"'"
       val IsCrossSite = if(value.getAs[String]("IsCrossSite")==null) "NULL" else value.getAs[String]("IsCrossSite").toInt
       val MovingCategoryCode = if(value.getAs[String]("MovingCategoryCode")==null) "NULL" else "'"+value.getAs[String]("MovingCategoryCode")+"'"
       val MovingReasonCode = if(value.getAs[String]("MovingReasonCode")==null) "NULL" else "'"+value.getAs[String]("MovingReasonCode")+"'"
       val IsApprove = if(value.getAs[String]("IsApprove")==null) "NULL" else value.getAs[String]("IsApprove").toInt
       val SwineTypeID = if(value.getAs[String]("SwineTypeID")==null) "NULL" else "'"+value.getAs[String]("SwineTypeID")+"'"
       val SwineID = if(value.getAs[String]("SwineID")==null) "NULL" else "'"+value.getAs[String]("SwineID")+"'"
       val FromSiteID = if(value.getAs[String]("FromSiteID")==null) "NULL" else "'"+value.getAs[String]("FromSiteID")+"'"
       val FromLocationID = if(value.getAs[String]("FromLocationID")==null) "NULL" else "'"+value.getAs[String]("FromLocationID")+"'"
       val ToSiteID = if(value.getAs[String]("ToSiteID")==null) "NULL" else "'"+value.getAs[String]("ToSiteID")+"'"
       val ToLocationID = if(value.getAs[String]("ToLocationID")==null) "NULL" else "'"+value.getAs[String]("ToLocationID")+"'"
       val SourceFormID = if(value.getAs[String]("SourceFormID")==null) "NULL" else "'"+value.getAs[String]("SourceFormID")+"'"
       val Remark = if(value.getAs[String]("Remark")==null) "NULL" else "'"+value.getAs[String]("Remark")+"'"
       val Status = if(value.getAs[String]("Status")==null) "NULL" else value.getAs[String]("Status").toInt
       val SiteID = if(value.getAs[String]("SiteID")==null) "NULL" else "'"+value.getAs[String]("SiteID")+"'"
       val CreatedDatetime = if(value.getAs[String]("CreatedDatetime")==null) "NULL" else "'"+value.getAs[String]("CreatedDatetime")+"'"
       val CreatedUserID = if(value.getAs[String]("CreatedUserID")==null) "NULL" else "'"+value.getAs[String]("CreatedUserID")+"'"
       val ModifiedDatetime = if(value.getAs[String]("ModifiedDatetime")==null) "NULL" else "'"+value.getAs[String]("ModifiedDatetime")+"'"
       val ModifiedUserID = if(value.getAs[String]("ModifiedUserID")==null) "NULL" else "'"+value.getAs[String]("ModifiedUserID")+"'"
       val CreatedOrigin = if(value.getAs[String]("CreatedOrigin")==null) "NULL" else "'"+value.getAs[String]("CreatedOrigin")+"'"
       val CreatedDeviceNo = if(value.getAs[String]("CreatedDeviceNo")==null) "NULL" else "'"+value.getAs[String]("CreatedDeviceNo")+"'"
       val ModifiedOrigin = if(value.getAs[String]("ModifiedOrigin")==null) "NULL" else "'"+value.getAs[String]("ModifiedOrigin")+"'"
       val ModifiedDeviceNo = if(value.getAs[String]("ModifiedDeviceNo")==null) "NULL" else "'"+value.getAs[String]("ModifiedDeviceNo")+"'"
       val Location = if(value.getAs[String]("Location")==null) "NULL" else "'"+value.getAs[String]("Location")+"'"
   
       statement.executeUpdate("Insert into raw_data_MI (ID,SeqNo,FormID,ExecDate,ExecEmpNo,MovingType,IsCrossSite,MovingCategoryCode,MovingReasonCode,IsApprove,SwineTypeID,SwineID,FromSiteID,FromLocationID,ToSiteID,ToLocationID,SourceFormID,Remark,Location,Status,SiteID,CreatedDatetime,CreatedUserID,ModifiedDatetime,ModifiedUserID,CreatedOrigin,CreatedDeviceNo,ModifiedOrigin,ModifiedDeviceNo) values ("+ID+","+SeqNo+","+FormID+","+ExecDate+","+ExecEmpNo+","+MovingType+","+IsCrossSite+","+MovingCategoryCode+","+MovingReasonCode+","+IsApprove+","+SwineTypeID+","+SwineID+","+FromSiteID+","+FromLocationID+","+ToSiteID+","+ToLocationID+","+SourceFormID+","+Remark+","+Location+","+Status+","+SiteID+","+CreatedDatetime+","+CreatedUserID+","+ModifiedDatetime+","+ModifiedUserID+","+CreatedOrigin+","+CreatedDeviceNo+","+ModifiedOrigin+","+ModifiedDeviceNo+")")
       if(Location=="A"){
         statement.executeUpdate("replace into status_data_MI (ID,SeqNo,FormID,ExecDate,ExecEmpNo,MovingType,IsCrossSite,MovingCategoryCode,MovingReasonCode,IsApprove,SwineTypeID,SwineID,FromSiteID,FromLocationID,ToSiteID,ToLocationID,SourceFormID,Remark,IsComplete,Status,SiteID,CreatedDatetime,CreatedUserID,ModifiedDatetime,ModifiedUserID,CreatedOrigin,CreatedDeviceNo,ModifiedOrigin,ModifiedDeviceNo) values ("+ID+","+SeqNo+","+FormID+","+ExecDate+","+ExecEmpNo+","+MovingType+","+IsCrossSite+","+MovingCategoryCode+","+MovingReasonCode+","+IsApprove+","+SwineTypeID+","+SwineID+","+FromSiteID+","+FromLocationID+","+ToSiteID+","+ToLocationID+","+SourceFormID+","+Remark+",0,"+Status+","+SiteID+","+CreatedDatetime+","+CreatedUserID+","+ModifiedDatetime+","+ModifiedUserID+","+CreatedOrigin+","+CreatedDeviceNo+","+ModifiedOrigin+","+ModifiedDeviceNo+")")
       }else{
         statement.executeUpdate("replace into status_data_MI (ID,SeqNo,FormID,ExecDate,ExecEmpNo,MovingType,IsCrossSite,MovingCategoryCode,MovingReasonCode,IsApprove,SwineTypeID,SwineID,FromSiteID,FromLocationID,ToSiteID,ToLocationID,SourceFormID,Remark,IsComplete,Status,SiteID,CreatedDatetime,CreatedUserID,ModifiedDatetime,ModifiedUserID,CreatedOrigin,CreatedDeviceNo,ModifiedOrigin,ModifiedDeviceNo) values ("+ID+","+SeqNo+","+FormID+","+ExecDate+","+ExecEmpNo+","+MovingType+","+IsCrossSite+","+MovingCategoryCode+","+MovingReasonCode+","+IsApprove+","+SwineTypeID+","+SwineID+","+FromSiteID+","+FromLocationID+","+ToSiteID+","+ToLocationID+","+SourceFormID+","+Remark+",1,"+Status+","+SiteID+","+CreatedDatetime+","+CreatedUserID+","+ModifiedDatetime+","+ModifiedUserID+","+CreatedOrigin+","+CreatedDeviceNo+","+ModifiedOrigin+","+ModifiedDeviceNo+")")
       }
     }
   
     override def close(errorOrNull: Throwable): Unit = {
       connection.close()
     }
   }
   
   val df = spark.readStream
     .format("kafka")
     .option("kafka.bootstrap.servers","192.168.1.114:9092")
     .option("startingOffsets", "latest")
     .option("subscribe","spark_kafka")
     .option("maxOffsetsPerTrigger", 10000)
     .load()
   
   val kafkaDF = df.selectExpr("CAST(value AS STRING)")
   
   val schema = new StructType()
     .add("ID",StringType)
     .add("SeqNo",StringType)
     .add("FormID",StringType)
     .add("ExecDate",StringType)
     .add("ExecEmpNo",StringType)
     .add("MovingType",StringType)
     .add("IsCrossSite",StringType)
     .add("MovingCategoryCode",StringType)
     .add("MovingReasonCode",StringType)
     .add("IsApprove",StringType)
     .add("SwineTypeID",StringType)
     .add("SwineID",StringType)
     .add("FromSiteID",StringType)
     .add("FromLocationID",StringType)
     .add("ToSiteID",StringType)
     .add("ToLocationID",StringType)
     .add("SourceFormID",StringType)
     .add("Remark",StringType)
     .add("Location",StringType)
     .add("Status",StringType)
     .add("SiteID",StringType)
     .add("CreatedDatetime",StringType)
     .add("CreatedUserID",StringType)
     .add("ModifiedDatetime",StringType)
     .add("ModifiedUserID",StringType)
     .add("CreatedOrigin",StringType)
     .add("CreatedDeviceNo",StringType)
     .add("ModifiedOrigin",StringType)
     .add("ModifiedDeviceNo",StringType)
   
   val kafkaDF2 = kafkaDF.select(from_json($"value", schema) as "data1").select($"data1.*")
   
   
   val url = "jdbc:mysql://192.168.1.115:3306/hidb?useSSL=false&useUnicode=true&characterEncoding=utf8"
   val user  = "root"
   val pwd = "mysql"
   val writer:ForeachWriter[Row] = new jdbcSink(url,user,pwd);
   val query = kafkaDF2
     .writeStream
     .foreach(writer)
   //  .format("console")
     .outputMode("update")
     .start()
   
   query.awaitTermination()
   
   
   
   ```

   





### MySQL



1. 建立新表格

   使用hidb 的 database

   ```
   use hidb;
   ```

2. 建立兩個新的 table

   raw_data 存放原始資料

   ```
   create table raw_data (date varchar(50) default null,tagId int(50) default null , antennaNo varchar(50) default null);
   ```

   status_data存放統計資料

   ```mysql
   create table status_data (date varchar(50) default null,tagId int(50) default null UNIQUE KEY, status varchar(50) default null);
   ```

3. 刪除table

   ```mysql
   truncate table status_data;
   ```





### Json data



1. 範例 json 資料

   ```json
   {
   	"records": [{
   		"date": "2019-8-8 10:10:00",
   		"tagId": "1",
   		"antennaNo": "A"
   	}, {
   		"date": "2019-8-8 10:13:00",
   		"tagId": "1",
   		"antennaNo": "A"
   	}, {
   		"date": "2019-8-8 10:14:00",
   		"tagId": "1",
   		"antennaNo": "B"
   	}, {
   		"date": "2019-8-8 10:12:00",
   		"tagId": "1",
   		"antennaNo": "A"
   	}, {
   		"date": "2019-8-8 10:20:00",
   		"tagId": "1",
   		"antennaNo": "B"
   	}, {
   		"date": "2019-8-8 10:17:00",
   		"tagId": "1",
   		"antennaNo": "B"
   	}, {
   		"date": "2019-8-8 10:22:00",
   		"tagId": "2",
   		"antennaNo": "A"
   	}, {
   		"date": "2019-8-8 10:30:00",
   		"tagId": "2",
   		"antennaNo": "B"
   	}, {
   		"date": "2019-8-8 10:25:00",
   		"tagId": "2",
   		"antennaNo": "B"
   	}, {
   		"date": "2019-8-8 10:39:00",
   		"tagId": "2",
   		"antennaNo": "A"
   	}, {
   		"date": "2019-8-8 10:34:00",
   		"tagId": "2",
   		"antennaNo": "B"
   	}]
   }
   ```

   






### Python 做欄位資料

1. 程式

   ```python
   import pymysql
   import pandas as pd
   import datetime as ti
   from datetime import datetime
   import random
   import numpy as np
   import requests
   import json
   
   db = pymysql.connect("192.168.1.115","root","mysql","hidb" )
   
   query = "select ID,SeqNo,FormID,ExecDate,ExecEmpNo,MovingType,IsCrossSite,MovingCategoryCode,MovingReasonCode,IsApprove,SwineTypeID,SwineID,FromSiteID,FromLocationID,ToSiteID,ToLocationID,SourceFormID,Remark,Status,SiteID,CreatedDatetime,CreatedUserID,ModifiedDatetime,ModifiedUserID,CreatedOrigin,CreatedDeviceNo,ModifiedOrigin,ModifiedDeviceNo from SwineLocationHistory"
   # query = "select ID,ExecDate,SwineID,ModifiedDatetime from SwineLocationHistory limit 1"
   
   df = pd.read_sql(query, con=db)
   # print(type(df.loc[0]['ExecDate']))
   
   df['ExecDate'] = df['ExecDate'].astype('int64')// 10**9
   # print(df)
   # print(df['ExecDate'].astype('int64')// 10**9)
   #convert to timestamp
   len_df = len(df)
   # for i in range(len_df):
   #     test_time =ti.datetime.strptime(df.loc[i]["ExecDate"],"%Y-%m-%d %H:%M:%S")
   #     timestamp = datetime.timestamp(test_time)
   #     df.loc[i,"ExecDate"]=timestamp
   #
   # make - 30s data
   df2 = pd.read_sql(query, con=db)
   df2['ExecDate'] = df2['ExecDate'].astype('int64')// 10**9
   time_30s = lambda x:x-30
   time_30s(df['ExecDate'])
   df_30s = df2
   df_30s['ExecDate']=time_30s(df_30s['ExecDate'])
   location_a = ["A"]*len_df
   df_30s["Location"] = location_a
   
   # make - 30s data
   df3 = pd.read_sql(query, con=db)
   df3['ExecDate'] = df3['ExecDate'].astype('int64')// 10**9
   time_10s = lambda x:x-10
   time_10s(df['ExecDate'])
   df_10s = df3
   df_10s['ExecDate']=time_10s(df_10s['ExecDate'])
   location_AB=[random.choice('AB') for _ in range(len_df)]
   df_10s["Location"] = location_AB
   
   location_b = ["B"]*len_df
   df["Location"] = location_b
   
   new_df = df.append([df_30s,df_10s])
   
   new_df = new_df.sort_values(by=['ExecDate', 'SwineID'])
   new_df['ExecDate']=pd.to_datetime(new_df['ExecDate'], unit='s')
   
   new_df['ExecDate'] = new_df.ExecDate.apply(str)
   
   json_data ={"records": new_df.to_dict('records')}
   
   def myconverter(o):
       if isinstance(o, ti.datetime):
           return o.__str__()
   
   headers = {"Content-Type":"application/json"}
   print(json_data)
   a=requests.post("http://192.168.1.112:7500/data_to_kafka",headers=headers,data=json.dumps(json_data,default=myconverter))
   
   print(a.status_code)
   ```

   